<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>升级操作系统 No.1</title>
      <link href="posts/f3e88753.html"/>
      <url>posts/f3e88753.html</url>
      
        <content type="html"><![CDATA[<h3 id="知人不评人"><a href="#知人不评人" class="headerlink" title="知人不评人"></a><strong>知人不评人</strong></h3><ul><li>轻易地给一个人下定论，对别人是一种伤害，更是一种不公平。</li></ul><h3 id="知事不声张"><a href="#知事不声张" class="headerlink" title="知事不声张"></a><strong>知事不声张</strong></h3><ul><li>知事不声张，既会保全别人的体面，又能周全自己的修养。</li><li>因为尊重，所以不拆穿；因为慈悲，所以不忍他难堪。 </li><li>人生在世，各有各的光鲜，各有各的苦衷。</li><li>不道破别人的心思，是将心比心的情商；不触碰别人的伤疤，是不动声色的善良。</li><li>知事不声张，对自己而言，是举手之劳；对别人而言，却是莫大的幸运。</li></ul><h3 id="知理不争辩"><a href="#知理不争辩" class="headerlink" title="知理不争辩"></a><strong>知理不争辩</strong></h3><ul><li>观点不同，相互尊重；层次不同，无需争辩。 </li><li>在不痛不痒的小事上，较高下，必定拉低自己的格局；在鸡毛蒜皮的琐碎上，辨输赢，终将耗费自己的心力。</li><li>用豁达化解是非，用沉默减少冲突。真正的聪明人都懂的：有一种豁达叫“知理不争辩”；有一种智慧叫“得理也饶人”。</li><li>《庄子》里说：“<strong>井蛙不可语海，夏虫不可语冰。</strong>”意思：对井里的蛙不可与它谈论关于海的事情，是由于它的眼界受到狭小居处的局限；对夏天生死的虫子不可谈论关于冰雪的事情，是由于它的眼界受到时令的制约。</li><li>喋喋不休，不如就此打住；斤斤计较，不如一笑而过。</li><li><strong>欲成大树，莫与草争。将军有剑，不斩苍蝇。</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 生活记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人间感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>test-mathjax</title>
      <link href="posts/1329187c.html"/>
      <url>posts/1329187c.html</url>
      
        <content type="html"><![CDATA[<p>$x\in X_1P_s(Y|X=x)=P_t(Y|X=x)$</p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习中的Normalization</title>
      <link href="posts/e7d825a5.html"/>
      <url>posts/e7d825a5.html</url>
      
        <content type="html"><![CDATA[<p>来源丨<a href="https://zhuanlan.zhihu.com/p/33173246">https://zhuanlan.zhihu.com/p/33173246</a></p><h2 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h2><p>  深度神经网络模型训练之难众所周知，其中一个重要的现象就是Internal Covariate Shift. Batch Norm 大法自2015年由Google提出之后，就成为深度学习必备之神器。自BN之后，Layer Norm/ Weight Norm/ Cosine Norm 等也横空出世。本文从Normalization的背景讲起，用一个公式概括Normalization的基本思想与通用框架，将各大主流方法一一对号入座进行深入的对比分析，并从参数和数据的伸缩不变性的角度探讨Normalization有效的深层原因。<br/></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>为什么需要Normalization<br>———深度学习中的 Internal Covariate Shift 问题极其影响</li><li>Normalization的通用框架与基本思想<br>———从主流Normalization方法中提炼出的抽象框架</li><li>主流 Normalization 方法梳理<br>———结合上述框架，将 BatchNorm/ LayerNorm/ WeightNorm/ CosineNorm 对号入座，各种方法之间的异同水落石出。</li><li>Normalization 为什么会有效？<br>———从参数和数据的伸缩不变性探讨Normalization有效的深层原因。<br>以下是正文，enjoy.</li></ol><h3 id="1-为什么需要Normalization"><a href="#1-为什么需要Normalization" class="headerlink" title="1. 为什么需要Normalization"></a>1. 为什么需要Normalization</h3><h4 id="1-1-独立同分布与白化"><a href="#1-1-独立同分布与白化" class="headerlink" title="1.1 独立同分布与白化"></a>1.1 独立同分布与白化</h4><p>  机器学习界的炼丹师们最喜欢的数据有什么特点？窃以为，莫过于“<strong>独立同分布</strong>”了, 即<em>independent and identically distributed</em>, 简称为 <em>i.i.d.</em> 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。</p><p>因此，在把数据喂给机器学习模型之前， “<strong>白化（whitening）</strong>”是一个重要的数据预处理步骤。白化一般包含两个目的：<br>（1）<em>去除特征之间的相关性</em>  —&gt; 独立；<br>（2）<em>使得所有特征具有相同的均值和方差</em>  —&gt; 同分布。</p><p>白化最典型的方法就是PCA， 可以参考阅读 PCA Whitening (<a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/</a>)</p><h4 id="1-2-深度学习中的Internal-Covariate-Shift"><a href="#1-2-深度学习中的Internal-Covariate-Shift" class="headerlink" title="1.2 深度学习中的Internal Covariate Shift"></a>1.2 深度学习中的Internal Covariate Shift</h4><p>深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致地参数更新策略。<br>Google将这一现象总结为Internal Covariate Shift， 简称ICS, 什么是ICS呢？</p><p>大家都知道统计机器学习中地一个经典假设是“<strong>源空间</strong>（source domain）和<strong>目标空间</strong>（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如transfer learning/ domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有$x\in X_1P_s(Y|X=x)=P_t(Y|X=x)$但是$P_s(X)\neq P_t(X)$ 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p><h4 id="1-3-ICS会导致什么问题？"><a href="#1-3-ICS会导致什么问题？" class="headerlink" title="1.3  ICS会导致什么问题？"></a>1.3  ICS会导致什么问题？</h4><p>简而言之，每个神经元的输入数据不再是“独立同分布”。<br>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。<br>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。<br>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能地谨慎。</p><h3 id="2-Normalization-的通用框架与基本思想"><a href="#2-Normalization-的通用框架与基本思想" class="headerlink" title="2. Normalization 的通用框架与基本思想"></a>2. Normalization 的通用框架与基本思想</h3><p>我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量$\textbf x=(x_1, x_2, …, x_d)$ 通过某种运算后，输出一个标量值：$y=f(\textbf x)$ 。</p><p>由于ICS问题的存在，$\textbf x$ 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作可微的，保证白化操作可以通过反向传播来更新梯度。</p><p>因此，以BN为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将$\textbf x$ 送给神经元之前，先对其做平移和伸缩变换，将$\textbf x$ 的分布规范化成在固定区间范围的标准分布。</p><p>通用变换框架就如下所示：</p><script type="math/tex; mode=display">h=f(g\cdot\frac{x-\mu}{\sigma}+b)</script><p>我们来看看这个公式中的各个参数。</p><p>（1）$\mu$是平移参数（shift parameter），$\sigma$是缩放参数（scale parameter）。通过这两个参数进行shift和scale变换：</p><script type="math/tex; mode=display">\hat{\textbf x}=\frac{\textbf x - \mu}{\sigma}</script><p>数据符合均值为0、方差为1的标准分布。<br>（2）$\textbf b$是再平移参数（re-shift parameter），$\textbf g$是再缩放参数（re-scale parameter）。将上一步得到的$\hat{\textbf x}$ 进一步变换为：</p><script type="math/tex; mode=display">\textbf y = \textbf g\cdot \hat{\textbf x} + \textbf b</script><p>最终得到的数据符合均值为$\textbf b$、方差为 $\textbf g^2$ 的分布。<br>奇不奇怪？奇不奇怪？</p><p>说好的处理ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？<br>答案是——<strong>为了保证模型的表达能力不因规范化而下降</strong></p><p>我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为0、方差为1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。<br>沮不沮丧？沮不沮丧？<br>难道我们底层神经元人民就在做无用功吗？ </p><p>所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为$\textbf b$、方差为$\textbf g^2$）。rescale和reshift的参数都是可学习的，这就使得Normalization层可以学习如何去尊重底层的学习结果。</p><p>除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化的能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。</p><p>那么问题又来了——<br><strong>经过这么的变回来再变过去，会不会跟没变一样？</strong></p><p>不会。因为，再变换引入的两个新参数$\textbf g$和$\textbf b$，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中，$\textbf x$的均值取决于下层神经网络的复杂关联；但在新参数中，$\textbf y = \textbf g \cdot \hat{\textbf x} + \textbf b$ 仅由</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Normalization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="posts/4a17b156.html"/>
      <url>posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Butterfly </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
